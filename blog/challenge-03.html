<!DOCTYPE html><!--  Last Published: Tue May 31 2022 08:33:01 GMT+0000 (Coordinated Universal Time)  -->
<html data-wf-page="627bca8c7c85efe341a4959c" data-wf-site="61f1998d59720032c9348551">
<head>
  <meta charset="utf-8">
  <title>Challenge 03</title>
  <meta content="Challenge 03" property="og:title">
  <meta content="Challenge 03" property="twitter:title">
  <meta content="width=device-width, initial-scale=1" name="viewport">
  <link href="../css/normalize.css" rel="stylesheet" type="text/css">
  <link href="../css/components.css" rel="stylesheet" type="text/css">
  <link href="../css/ab9372gr9foeinkdxl.css" rel="stylesheet" type="text/css">
  <script src="https://ajax.googleapis.com/ajax/libs/webfont/1.6.26/webfont.js" type="text/javascript"></script>
  <script type="text/javascript">WebFont.load({  google: {    families: ["Poppins:regular,italic,600,600italic,900,900italic"]  }});</script>
  <!-- [if lt IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js" type="text/javascript"></script><![endif] -->
  <script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
  <link href="../images/favicon.png" rel="shortcut icon" type="image/x-icon">
  <link href="../images/webclip.png" rel="apple-touch-icon">
</head>
<body>
  <div class="title-section wf-section">
    <div class="title-section-wrapper">
      <h1 class="hero-h2">Week 15</h1>
      <h1 class="hero-h1">Challenge 03</h1>
      <h3 class="hero-h3">May 06 2022</h3>
      <div class="hero-button-wrapper">
        <a href="#Content" class="button primary w-button">Read more</a>
      </div>
    </div>
  </div>
  <div id="Content" class="article-content wf-section">
    <div class="container w-container">
      <h3 class="h3">Idea</h3>
      <p class="paragraph">Together with Jeremy, I wanted to build a generative storytelling system, including some procedural visuals to go along with it, running on the Raspberry Pi Zero W.</p>
      <div class="image-wrapper"><img src="../images/1.png" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, 96vw" srcset="../images/1-p-500.png 500w, ../images/1-p-800.png 800w, ../images/1-p-1080.png 1080w, ../images/1-p-1600.png 1600w, ../images/1-p-2000.png 2000w, ../images/1.png 2064w" alt="" class="image contain"></div>
      <p class="image-credits">The signal chain and components visualized in an easy-to-understand way. </p>
      <h3 class="h3">Conceptual Structure</h3>
      <div class="image-wrapper"><img src="../images/3.png" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, 96vw" srcset="../images/3-p-500.png 500w, ../images/3-p-800.png 800w, ../images/3-p-1080.png 1080w, ../images/3-p-1600.png 1600w, ../images/3-p-2000.png 2000w, ../images/3.png 2374w" alt="" class="image contain"></div>
      <p class="image-credits">Programming flowchart showing the architecture of what would eventually grow into separate python-objects. </p>
      <h3 class="h3">Timetable</h3>
      <div class="image-wrapper"><img src="../images/2.png" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, 96vw" srcset="../images/2-p-500.png 500w, ../images/2-p-800.png 800w, ../images/2-p-1080.png 1080w, ../images/2-p-1600.png 1600w, ../images/2-p-2000.png 2000w, ../images/2-p-2600.png 2600w, ../images/2.png 2720w" alt="" class="image contain"></div>
      <p class="image-credits">Our timetable for the one week, including deliverables. This disregarded the tact </p>
      <h3 class="h3">Execution</h3>
      <p class="paragraph">In the end, the project consisted of two separate components: the image generator and the story generator. </p>
      <h3 class="h4">Story Generator</h3>
      <p class="paragraph">The story generator consisted of a python script employing <a href="https://pypi.org/project/beautifulsoup4/" target="_blank">BautifulSoup4</a> to crawl websites and extract HTML data from it. By going through all the <em>&lt;a&gt;</em> elements on a website, it extracts all external links and filters it for undesired ones (such as social media and Google links). After extracting the first <em>&lt;h2&gt;</em> element, it chooses a random external link and navigates to it. This way the script can autonomously navigate the web and jump from page to page, as long as he visited websites have at least one external link. <br><br>This way, every website will provide a couple keywords that can be used as prompts for GPT-3, a natural language generation network by OpenAI. We send the keywords via an API call to the network, prompting a short story to return. After an average of five seconds, a multi-line, sometimes multi-paragraph short story is returned, revolving around the provided keywords. This is an example, based on &#x27;books&#x27; and &#x27;dolphins&#x27;:<br>‍<br><em>&quot;Once upon a time, there was a beautiful kingdom made entirely of books. The shelves were lined with all sorts of stories, from classic fairytales to modern-day adventures. The people of this kingdom loved to read, and they would often spend hours curled up with their favorite book. One day, a terrible storm swept through the kingdom, destroying everything in its path. All of the books were blown away, and the once- lively kingdom was left in ruins. The people of the kingdom were heartbroken. They didn&#x27;t know what to do without their beloved books. But then, one day, a strange woman appeared and said she could help them. She had magical powers that could bring the books back to life. With the woman&#x27;s help, the kingdom was soon restored to its former glory. And everyone lived happily ever after surrounded by their favorite stories once again.&quot;<br><br></em>Based on this relatively simple structure, a procedural storytelling algorithm evolves. <em><br></em></p>
      <div class="image-wrapper"><img src="../images/Bildschirmfoto-2022-05-11-um-17.31.34.png" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, 96vw" srcset="../images/Bildschirmfoto-2022-05-11-um-17.31.34-p-500.png 500w, ../images/Bildschirmfoto-2022-05-11-um-17.31.34-p-800.png 800w, ../images/Bildschirmfoto-2022-05-11-um-17.31.34-p-1080.png 1080w, ../images/Bildschirmfoto-2022-05-11-um-17.31.34-p-1600.png 1600w, ../images/Bildschirmfoto-2022-05-11-um-17.31.34-p-2000.png 2000w, ../images/Bildschirmfoto-2022-05-11-um-17.31.34-p-2600.png 2600w, ../images/Bildschirmfoto-2022-05-11-um-17.31.34-p-3200.png 3200w, ../images/Bildschirmfoto-2022-05-11-um-17.31.34.png 3360w" alt="" class="image"></div>
      <p class="image-credits">The development environment including a sample of the text output (right). </p>
      <h3 class="h4">Image Generator</h3>
      <p class="paragraph">The image generator consisted of a <a href="https://github.com/borisdayma/dalle-mini" target="_blank">Dalle-E mini</a> instance running on Google Colab. This was accessed through an API to prompt the network for the image generation and then receive the raw image data back. To limit data throughput and speed up the generation, we fixed the resolution to 256x256 pixels. By doing this, we managed to keep the response time consistently below ten seconds. The data stream back consisted of encoded in base46 raw image codec. With the help of Pietro, we then transformed that into png format and then into bmp format in order to be integrated in the thermal printout. </p>
      <div class="image-wrapper"><img src="../images/generated0.jpeg" loading="lazy" alt="" class="image"><img src="../images/generated1.jpeg" loading="lazy" alt="" class="image"></div>
      <p class="image-credits">Two examples generated by Dalle-E mini on the prompt &quot;Cherry Trees&quot;.</p>
      <h3 class="h4">Hardware Output</h3>
      <p class="paragraph">To interface with the real world, two tools were chosen: Input via the command line and a physical printout via a thermal printer, normally used for printing receipts. Through the command line, the user can specify the number of jumps and the starting URL. This is the primary input the user can do. As for the output, the links, keywords and generated story are printed out via the thermal printer, providing a continuous log of the procedural storytelling. To interface with the printer is the next immediate step to take. Right now, the text output happens via the command line. </p>
      <div class="image-wrapper"><img src="../images/6.jpeg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, 96vw" srcset="../images/6-p-500.jpeg 500w, ../images/6-p-800.jpeg 800w, ../images/6-p-1080.jpeg 1080w, ../images/6-p-1600.jpeg 1600w, ../images/6-p-2000.jpeg 2000w, ../images/6-p-2600.jpeg 2600w, ../images/6-p-3200.jpeg 3200w, ../images/6.jpeg 4032w" alt="" class="image"></div>
      <p class="image-credits">The thermal printer connected to the Raspberry Pi Zero via UART. </p>
      <h3 class="h3">Next Steps</h3>
      <p class="paragraph">- Finish Interfacing with the thermal printer<br>- Integrate image generation into story generator<br>- Design a protocol to deliver text and Bitmaps from the Pi to the printer<br>- Build an enclosure</p>
      <div class="image-wrapper"><img src="../images/4.jpeg" loading="lazy" sizes="(max-width: 479px) 96vw, (max-width: 767px) 95vw, 96vw" srcset="../images/4-p-500.jpeg 500w, ../images/4-p-800.jpeg 800w, ../images/4-p-1080.jpeg 1080w, ../images/4-p-1600.jpeg 1600w, ../images/4-p-2000.jpeg 2000w, ../images/4-p-2600.jpeg 2600w, ../images/4-p-3200.jpeg 3200w, ../images/4.jpeg 4032w" alt="" class="image"></div>
      <p class="image-credits">Jeremy debugging the run environment on the Raspberry Pi Zero. </p>
      <h3 class="h3">My Reflection</h3>
      <p class="paragraph">Ultimately, the image generator should be integrated in the storytelling algorithm to provide pictures on the basis of the keywords extracted by the web crawler. This would effectively wrap all of these components into a single content producing program. To get there is my personal mid-term goal. This might be achieved until the end of the semester, but it might as well be finished only after it. Regardless, it will be finished eventually this week and hopefully grown into a published project. <br><br>Isolated from the fact that this project was not completely finished within a single week, this Fab Challenge was of critical importance for me personally. It was the first time writing Python in a bigger scale and structuring it into dedicated objects. This implied an understanding of software architecture, which I didn&#x27;t have any other touch points with before. Additionally, doing API calls and processing JSON replies in Python, was a new thing for me.<br><br>In this sense this week&#x27;s challenge was hugely inspirational for me, since for the first time, I could see hoe Python could serve as a bridge between different systems, interfacing between them and post-processing data. This way, software platforms increasingly reveal themselves to me not as silos with isolated environments, but simply as inputs and outputs to be combined. </p>
      <h3 class="h3">Github Repository</h3>
      <p class="paragraph">All our code, including documentation, can be found <a href="https://github.com/chris-ernst/fabacademy-challenge-3" target="_blank">here</a>. </p>
    </div>
  </div>
  <div class="footer wf-section">
    <div class="footer-wrapper">
      <p class="footer-text">2022 – Christian Ernst</p>
    </div>
  </div>
  <script src="https://d3e54v103j8qbb.cloudfront.net/js/jquery-3.5.1.min.dc5e7f18c8.js?site=61f1998d59720032c9348551" type="text/javascript" integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin="anonymous"></script>
  <script src="../js/ab9372gr9foeinkdxl.js" type="text/javascript"></script>
  <!-- [if lte IE 9]><script src="https://cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif] -->
</body>
</html>